---
title: Geotutorial: Location Extraction using Supervised Models (Notebook-1)
author: Hussein S. Al-Olimat
source: "github.com/halolimat"

---

# Geotutorial: Location Extraction using Supervised Models (Notebook-1)

### Dataset we will be using:

#### 2017 The 3rd Workshop on Noisy User-generated Text (W-NUT): Emerging and Rare Entities task: http://noisy-text.github.io/2017/


```python
# Data sources
WNUT17_dev = "https://raw.githubusercontent.com/halolimat/NER-WNUT17/master/data/emerging.dev.conll"
WNUT17_train = "https://raw.githubusercontent.com/halolimat/NER-WNUT17/master/data/emerging.train.conll"
WNUT17_test = "https://raw.githubusercontent.com/halolimat/NER-WNUT17/master/data/emerging.test.conll"
```

### Read the data into python list


```python
import urllib2

# Read file into a list after removing '\n' character from all lines
WNUT17_dev = [unicode(x.replace("\n","").decode("utf8")) for x in urllib2.urlopen(WNUT17_dev).readlines()]
WNUT17_train = [unicode(x.replace("\n","").decode("utf8")) for x in urllib2.urlopen(WNUT17_train).readlines()]
WNUT17_test = [unicode(x.replace("\n","").decode("utf8")) for x in urllib2.urlopen(WNUT17_test).readlines()]
```

### Let's peak at the data:
Tagged using [**Insideâ€“outsideâ€“beginning encoding**](http://link.hussein.space/iob_encoding)


```python
for line in WNUT17_dev[13:23]:
    print line
```

    You	O
    should	O
    '	O
    ve	O
    stayed	O
    on	O
    Redondo	B-location
    Beach	I-location
    Blvd	I-location
    .	O


----
#### Definitions of some of the entity classes in WNUT-17:

- **Location** :
    * Names that are locations (e.g. France). Don't mark locations that don't have their own name. Include punctuation in the middle of names. Fictional locations can be included, as long as they're referred to by name (e.g. "Hogwarts").
    * Examples:
        * "There was a celebration in London"
        * "The room** is empty" Wrong, because room isn't the name of a particular location


- **Corporation** :
    * Names of corporations (e.g. Google). Don't mark locations that don't have their own name. Include punctuation in the middle of names.
    * Examples:
        * "Stock in Tesla is soaring"
        * "Just taken my son to KFC** for dinner #greasybutgood" - considered not correct, not referring to Corporation, instead to a place.


- Person

- Product

- Creative work

- Group

------

#### Remove all other classes than "Location"


```python
# keeps only one class of our choice in all lines of the labeled dataset
# e.g., "RIT B-corporation" -> "RIT O"   and    "Hussein B-person" -> "Hussein O"
def keep_class(c, l):
    if len(l.strip()) == 0:
        return ""
    else:
        word = l.split("\t")[0]
        tag = l.split("\t")[1]

        if c not in tag:
            tag = "O"

        return word + "\t" + tag
```


```python
# Before removing other classes
for line in WNUT17_dev[41:51]:
    print line
```

    watching	O
    Rick	B-creative-work
    and	I-creative-work
    Morty	I-creative-work
    ðŸ˜‚	O

    wow	O
    emma	B-person
    and	O
    kaite	B-person



```python
WNUT17_dev = [keep_class("location", l) for l in WNUT17_dev]
WNUT17_train = [keep_class("location", l) for l in WNUT17_train]
WNUT17_test = [keep_class("location", l) for l in WNUT17_test]
```


```python
# After removing other classes
for line in WNUT17_dev[41:51]:
    print line
```

    watching	O
    Rick	O
    and	O
    Morty	O
    ðŸ˜‚	O

    wow	O
    emma	O
    and	O
    kaite	O


---
# Now, let's train a model using spaCy

#### We need to convert the training data to the correct format:

Convert WNUT17 files that are in the CoNLL-2003 format into JSON format for use with spaCy's
    train cli.


```python
import spacy
from spacy.gold import iob_to_biluo, GoldParse

def convert_conll_to_json(labeled_data):

    # convert labeled data into a string to tokenize it into docs
    labeled_data_str = "\n".join(labeled_data)
    labeled_data_docs = labeled_data_str.split("\n\n")

    output_docs = []

    for d in labeled_data_docs:
        words, iob_ents = zip(*[line.split('\t') for line in d.split("\n")])
        biluo_ents = iob_to_biluo(iob_ents)

        output_docs.append({
            'id': len(output_docs),
            'paragraphs': [{'sentences': [{'tokens': [{'orth': w, 'ner': ent} for (w, ent) in zip(words, biluo_ents) ]}]}]
        })

    return output_docs
```


```python
WNUT17_dev_json = convert_conll_to_json(WNUT17_dev)
WNUT17_train_json = convert_conll_to_json(WNUT17_train)
WNUT17_test_json = convert_conll_to_json(WNUT17_test)
```


```python
print WNUT17_dev_json[:1]
print
print WNUT17_train_json[:1]
print
print WNUT17_test_json[:1]
```

    [{'id': 0, 'paragraphs': [{'sentences': [{'tokens': [{'ner': u'O', 'orth': u'Stabilized'}, {'ner': u'O', 'orth': u'approach'}, {'ner': u'O', 'orth': u'or'}, {'ner': u'O', 'orth': u'not'}, {'ner': u'O', 'orth': u'?'}, {'ner': u'O', 'orth': u'That'}, {'ner': u'O', 'orth': u'\xb4'}, {'ner': u'O', 'orth': u's'}, {'ner': u'O', 'orth': u'insane'}, {'ner': u'O', 'orth': u'and'}, {'ner': u'O', 'orth': u'good'}, {'ner': u'O', 'orth': u'.'}]}]}]}]

    [{'id': 0, 'paragraphs': [{'sentences': [{'tokens': [{'ner': u'O', 'orth': u'@paulwalk'}, {'ner': u'O', 'orth': u'It'}, {'ner': u'O', 'orth': u"'s"}, {'ner': u'O', 'orth': u'the'}, {'ner': u'O', 'orth': u'view'}, {'ner': u'O', 'orth': u'from'}, {'ner': u'O', 'orth': u'where'}, {'ner': u'O', 'orth': u'I'}, {'ner': u'O', 'orth': u"'m"}, {'ner': u'O', 'orth': u'living'}, {'ner': u'O', 'orth': u'for'}, {'ner': u'O', 'orth': u'two'}, {'ner': u'O', 'orth': u'weeks'}, {'ner': u'O', 'orth': u'.'}, {'ner': u'B-location', 'orth': u'Empire'}, {'ner': u'I-location', 'orth': u'State'}, {'ner': u'L-location', 'orth': u'Building'}, {'ner': u'O', 'orth': u'='}, {'ner': u'U-location', 'orth': u'ESB'}, {'ner': u'O', 'orth': u'.'}, {'ner': u'O', 'orth': u'Pretty'}, {'ner': u'O', 'orth': u'bad'}, {'ner': u'O', 'orth': u'storm'}, {'ner': u'O', 'orth': u'here'}, {'ner': u'O', 'orth': u'last'}, {'ner': u'O', 'orth': u'evening'}, {'ner': u'O', 'orth': u'.'}]}]}]}]

    [{'id': 0, 'paragraphs': [{'sentences': [{'tokens': [{'ner': u'O', 'orth': u'&'}, {'ner': u'O', 'orth': u'gt'}, {'ner': u'O', 'orth': u';'}, {'ner': u'O', 'orth': u'*'}, {'ner': u'O', 'orth': u'The'}, {'ner': u'O', 'orth': u'soldier'}, {'ner': u'O', 'orth': u'was'}, {'ner': u'O', 'orth': u'killed'}, {'ner': u'O', 'orth': u'when'}, {'ner': u'O', 'orth': u'another'}, {'ner': u'O', 'orth': u'avalanche'}, {'ner': u'O', 'orth': u'hit'}, {'ner': u'O', 'orth': u'an'}, {'ner': u'O', 'orth': u'army'}, {'ner': u'O', 'orth': u'barracks'}, {'ner': u'O', 'orth': u'in'}, {'ner': u'O', 'orth': u'the'}, {'ner': u'O', 'orth': u'northern'}, {'ner': u'O', 'orth': u'area'}, {'ner': u'O', 'orth': u'of'}, {'ner': u'U-location', 'orth': u'Sonmarg'}, {'ner': u'O', 'orth': u','}, {'ner': u'O', 'orth': u'said'}, {'ner': u'O', 'orth': u'a'}, {'ner': u'O', 'orth': u'military'}, {'ner': u'O', 'orth': u'spokesman'}, {'ner': u'O', 'orth': u'.'}]}]}]}]



```python
import spacy
from spacy.gold import GoldParse
from spacy.tokens.doc import Doc

def train_model(labeled_data, n_iter=20):
    """Set up the pipeline and entity recognizer, and train the new entity."""

    # create blank Language class    
    nlp = spacy.blank('en')  # create blank Language class
    print("Created blank 'en' model")

    # Add entity recognizer to model
    # nlp.create_pipe works for built-ins that are registered with spaCy
    ner = nlp.create_pipe('ner')
    nlp.add_pipe(ner)

    ner.add_label("location")   # add new entity label to entity recognizer

    # convert labeled data into a string to tokenize it into docs
    labeled_data_str = "\n".join(labeled_data)
    labeled_data_docs = labeled_data_str.split("\n\n")

    print "Training model: iterations #: ",

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(n_iter):

            print itn,

            for d in labeled_data_docs:
                losses = {}

                words, iob_ents = zip(*[line.split('\t') for line in d.split("\n")])
                doc = Doc(nlp.vocab, words)

                gold = GoldParse(doc, entities=iob_ents)
                nlp.update([doc], [gold], drop=0.5, sgd=optimizer, losses=losses)

                #print(losses)
        print


    location_model.to_disk("location_model_spacy.ner")

    return nlp
```


```python
#location_model = train_model(WNUT17_dev)
location_model = spacy.load("location_model_spacy.ner")
```


```python
# test the trained model
test_text = u'Are you attending ISCRAM conference in London?'
#test_text = u'I am staying at Redondo Beach Blvd'
doc = location_model(test_text)
print("Entities in '%s'" % test_text)
for ent in doc.ents:
    print(ent.label_, ent.text)
```

    Entities in 'Are you attending ISCRAM conference in London?'
    (u'location', u'London?')


---
## Now, let'e evaluate the trained model on the testing set


```python
from spacy.scorer import Scorer

def evaluate(ner_model, labeled_data):

    nlp = spacy.blank('en')  # create blank Language class

    # convert labeled data into a string to tokenize it into docs
    labeled_data_str = "\n".join(labeled_data)
    labeled_data_docs = labeled_data_str.split("\n\n")

    scorer = Scorer()

    for d in labeled_data_docs:
        words, iob_ents = zip(*[line.split('\t') for line in d.split("\n")])

        # Tokenizers are different therefore will cause a problem. Following is one of the examples:
        #words = [x.replace("-", "") for x in words]

        #doc = Doc(nlp.vocab, words)
        doc = " ".join(words)

        doc_gold_text = ner_model.make_doc(unicode(doc))

        try:
            gold = GoldParse(doc_gold_text, entities=iob_ents)

            pred_value = ner_model(unicode(doc))

            print pred_value.ents


            scorer.score(pred_value, gold)
        except:
            # ignore the problems caused by the tokenizer due to how our data was tokenized
            pass

    return scorer.scores
```


```python
evaluate(location_model, WNUT17_test)
```

    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Europe has n ',)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Ireland .,)
    ()
    ()
    ()
    ()
    ()
    (Europe,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Ireland,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Europe but it ',)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Earth ?,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Universe complies to such,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (Discovery ?,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (TRABAHO KA NA BA,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (United Kingdom https://t.co/WcGf0C68Vz,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    (White House on January 20,)
    ()
    ()
    ()
    ()
    (White House . The damage,)
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()
    ()





    {u'ents_f': 0.0,
     u'ents_p': 0.0,
     u'ents_r': 0.0,
     u'las': 0.0,
     u'tags_acc': 0.0,
     u'token_acc': 100.0,
     u'uas': 0.0}
